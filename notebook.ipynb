{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(message):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=message,\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(question: str, context: str):\n",
    "\n",
    "    if context == None or context == \"\":\n",
    "        prompt = f\"\"\"Give a detailed answer to the following question. Question: {question}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Using the information contained in the context, give a detailed answer to the question.\n",
    "            Context: {context}.\n",
    "            Question: {question}\"\"\"\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        # { \"role\": \"model\", \"content\": \"Recurrent Attention (RAG)** is a novel neural network architecture specifically designed\" }\n",
    "    ]\n",
    "\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assisstent that knows everything about Mazi Prima .\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},]\n",
    "    return get_completion(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am a helpful virtual assistant here to assist you with any questions or tasks you may have. How can I help you today?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(\"Hello!\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Loading and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loaders = [\n",
    "    PyPDFLoader(\"dataset/about_me.pdf\"),\n",
    "]\n",
    "pages = []\n",
    "for loader in loaders:\n",
    "    pages.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=128, chunk_overlap=12)\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(\"docs.npy\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = np.load(\"dataset/docs.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.embeddings import (\n",
    "    HuggingFaceEmbeddings\n",
    ")\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "encoder = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L12-v2', model_kwargs = {'device': \"cpu\"})\n",
    "faiss_db = FAISS.from_documents(docs, encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = encoder.embed_query(\"How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(question):\n",
    "    retrieved_docs = faiss_db.similarity_search(question, k=5)\n",
    "    context = \"\".join(doc.page_content + \"\\n\" for doc in retrieved_docs)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy. One unique challenge was the \n",
      "management's demand for early prediction capabilities, whi ch the existing model couldn't \n",
      "provide. Mazi addressed this by creating a new model that could predict churn earlier by \n",
      "improving the dataset.  \n",
      "Notable Projects at Vidio:  \n",
      "• Recommendation System : Designed and implemented a collaborative filtering \n",
      "recommendation system using cosine similarity, which improved user engagement \n",
      "metrics significantly.  \n",
      " the finance sector to summarize and analyze new and updated \n",
      "regulat ions, thereby aiding GRC analysts in streamlining their workflow and automating \n",
      "analysis to inform the team faster.  \n",
      "Notable Projects at Metrodata:  \n",
      "• HR Chatbot Development : Led the development of a chatbot using Microsoft Azure \n",
      "Open AI for the HR department.  This project involved integrating NLP models to \n",
      "answer employee queries about company policies, resulting in a 30% reduction in HR \n",
      "response time.  \n",
      "• Regulation Analysis Tool : Developed a Generative AI tool for the finance industry \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you tell me about a specific project at Vidio?\"\n",
    "retrieved_docs = faiss_db.similarity_search(question, k=2)\n",
    "context = \"\".join(doc.page_content + \"\\n\" for doc in retrieved_docs)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mazi holds a Bachelor of Science degree in Mathematics from Institut Teknologi Bandung.\n"
     ]
    }
   ],
   "source": [
    "question = \"Whats her bachelors degree?\"\n",
    "context = get_context(question)\n",
    "print(inference(question=question, context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_huggingface as a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'langchain_huggingface' from 'c:\\\\Users\\\\mazpr\\\\anaconda3\\\\envs\\\\openai\\\\lib\\\\site-packages\\\\langchain_huggingface\\\\__init__.py'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
