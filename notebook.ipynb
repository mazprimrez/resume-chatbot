{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mazpr\\anaconda3\\envs\\openai\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mazpr\\.cache\\huggingface\\hub\\models--google--canine-c. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "ACCESS_TOKEN = os.getenv(\"ACCESS_TOKEN\") # reads .env file with ACCESS_TOKEN=<your hugging face access token>\n",
    "\n",
    "model_id = \"Datascience-Lab/GPT2-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=ACCESS_TOKEN)\n",
    "# quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             token=ACCESS_TOKEN,\n",
    "                                             trust_remote_code=True\n",
    "                                             )\n",
    "model.eval()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(message):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=message,\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(question: str, context: str):\n",
    "\n",
    "    if context == None or context == \"\":\n",
    "        prompt = f\"\"\"Give a detailed answer to the following question. Question: {question}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Using the information contained in the context, give a detailed answer to the question.\n",
    "            Context: {context}.\n",
    "            Question: {question}\"\"\"\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        # { \"role\": \"model\", \"content\": \"Recurrent Attention (RAG)** is a novel neural network architecture specifically designed\" }\n",
    "    ]\n",
    "\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assisstent that knows everything about Mazi Prima .\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},]\n",
    "    return get_completion(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am a helpful virtual assistant here to assist you with any questions or tasks you may have. How can I help you today?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(\"Hello!\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Loading and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loaders = [\n",
    "    PyPDFLoader(\"dataset/about_me.pdf\"),\n",
    "]\n",
    "pages = []\n",
    "for loader in loaders:\n",
    "    pages.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=128, chunk_overlap=12)\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(\"docs.npy\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = np.load(\"docs.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mazpr\\anaconda3\\envs\\openai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\mazpr\\anaconda3\\envs\\openai\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from langchain_community.embeddings import (\n",
    "    HuggingFaceEmbeddings\n",
    ")\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "encoder = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L12-v2', model_kwargs = {'device': \"cpu\"})\n",
    "faiss_db = FAISS.from_documents(docs, encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = encoder.embed_query(\"How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(question):\n",
    "    retrieved_docs = faiss_db.similarity_search(question, k=5)\n",
    "    context = \"\".join(doc.page_content + \"\\n\" for doc in retrieved_docs)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " complex topics accessible to young students. In smaller groups or \n",
      "one-on-one settings, students were more comfortable asking questions, significantly \n",
      "enhancing their learning experience.  \n",
      "Education:  \n",
      "Mazi holds a Bachelor of Science degree in Mathematics from Institut Teknologi Bandung, \n",
      "where he completed his und ergraduate thesis on \"ASABRI and JIWASRAYA Stock Portfolio \n",
      "Optimization using Adaptive Spiral Optimization Method,\" achieving a GPA of 3.16 out of \n",
      "4.00.  \n",
      "Technical Skills:  \n",
      "His technical\n",
      ".  \n",
      "Technical Skills:  \n",
      "His technical skills encompass a wide range of tools and technologies, including Pyth on (with \n",
      "libraries such as Dash Plotly, Streamlit, Pandas, Sklearn, Keras, SHAP, Pytest, Flask, and \n",
      "more), PySpark, SQL, Open AI, GCP, Azure Machine Learning, Azure Open AI, Tableau, \n",
      "and Power BI. His expertise in data analysis includes statistical analyti cs, data preparation, \n",
      "data visualization, and dashboard building. In data modeling, Mazi excels in developing \n",
      "recommend\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What's her bachelors degree?\"\n",
    "retrieved_docs = faiss_db.similarity_search(question, k=2)\n",
    "context = \"\".join(doc.page_content + \"\\n\" for doc in retrieved_docs)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mazi holds a Bachelor of Science degree in Mathematics from Institut Teknologi Bandung.\n"
     ]
    }
   ],
   "source": [
    "question = \"Whats her bachelors degree?\"\n",
    "context = get_context(question)\n",
    "print(inference(question=question, context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  \\nMazi aims to become a T-shaped Data Scientist, with deep expertise in NLP, an area he is \\nparticularly passionate about and wishes to explore further.  \\nPersonal Interests:  \\nBeyond his technical capabilities, Mazi is a proactive and innovative professional who \\nconsistently seeks to leverage data to drive business success and improve operational \\nefficiencies. His interests include reading and hiking, which provide him with a well -rounded \\nperspective and a balanced approach to both his professional and personal life.  \\nFor more inform ation about Mazi Prima Re\\nMazi Prima Reza - Data Scientist  \\nMazi Prima Reza is a skilled Data Scientist with over three years of experience in the data \\nfield, currently working at Metrodata in Jakarta, Indonesia. His professional journey is \\nmarked by a strong background in handling diverse and end -to-end projects, ranging from \\ndata analysis to the implementation of advanced generative AI technologies. Maz i is \\ndedicated to building innovative solutions that automate repetitive tasks and enhance user \\nexperiences through cutting -edge data analytics.  \\nIn his current role at Metrodata, Mazi has successfully\\n.  \\nTechnical Skills:  \\nHis technical skills encompass a wide range of tools and technologies, including Pyth on (with \\nlibraries such as Dash Plotly, Streamlit, Pandas, Sklearn, Keras, SHAP, Pytest, Flask, and \\nmore), PySpark, SQL, Open AI, GCP, Azure Machine Learning, Azure Open AI, Tableau, \\nand Power BI. His expertise in data analysis includes statistical analyti cs, data preparation, \\ndata visualization, and dashboard building. In data modeling, Mazi excels in developing \\nrecommend\\n modeling, Mazi excels in developing \\nrecommendation systems, supervised and unsupervised machine learning, user segmentation, \\nprediction, NLP, time series analysis, Open AI, and expla inable AI. For instance, he used \\nPandas for data analysis and processing, Azure for environment connectivity, Open AI for \\nautomating statements, Streamlit for creating front -end applications, and PySpark for \\nprocessing large datasets.  \\nProfessional Developm ent: \\nseries analysis to predict monthly subscription trends played a crucial role in the c ompany's \\nstrategic planning efforts.  \\nTeaching and Mentoring:  \\nIn addition to his professional experience, Mazi has a strong passion for teaching and \\nmentoring. He began his teaching journey as a volunteer Data Science Mentor with \\nGeneration Girls, where he inspired and guided young learners. Building on this experience, \\nhe collaborated with Generation Girls and the Ministry of Communication and Informatics \\n(Kemneterian Komunikasi dan Informatika) to mentor elementary school students both\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
